#!/usr/bin/env python3
"""
Generate Performance Evaluation Table (Table 16) for Thesis
This script evaluates the model and generates detailed metrics for each disease class
"""
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import json
from datetime import datetime

def generate_performance_table():
    """Generate comprehensive performance metrics for all disease classes"""
    print("=" * 80)
    print("üîç GENERATING PERFORMANCE EVALUATION TABLE (Table 16)")
    print("=" * 80)
    
    # Load model
    model_path = "backend/trained_model_fito.h5"
    if not os.path.exists(model_path):
        print(f"‚ùå Model not found at: {model_path}")
        return
    
    print(f"\n‚úÖ Loading model from: {model_path}")
    model = tf.keras.models.load_model(model_path)
    
    # Load validation data
    val_path = r"C:\Users\HYUDADDY\Desktop\DATASET\tomato leaf diseases dataset(augmented)\validation"
    
    if not os.path.exists(val_path):
        print(f"‚ùå Validation data not found at: {val_path}")
        print("Please update the path to your validation dataset.")
        return
    
    # Create data generator (matching training configuration)
    val_datagen = ImageDataGenerator(rescale=1./255)
    val_generator = val_datagen.flow_from_directory(
        val_path,
        target_size=(192, 192),  # Updated to match training config
        batch_size=20,
        class_mode='categorical',
        shuffle=False
    )
    
    print(f"‚úÖ Loaded validation data: {val_generator.samples} samples")
    print(f"üìä Number of classes: {len(val_generator.class_indices)}")
    
    # Get predictions
    print("\nüß™ Running predictions on validation set...")
    predictions = model.predict(val_generator, verbose=1)
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = val_generator.classes
    
    # Get class names
    class_names = list(val_generator.class_indices.keys())
    
    # Calculate overall accuracy
    overall_accuracy = np.mean(predicted_classes == true_classes)
    
    # Calculate per-class metrics
    precision, recall, f1, support = precision_recall_fscore_support(
        true_classes, 
        predicted_classes, 
        labels=range(len(class_names)),
        zero_division=0
    )
    
    # Calculate per-class accuracy
    class_accuracies = []
    for i in range(len(class_names)):
        class_mask = true_classes == i
        if np.sum(class_mask) > 0:
            class_accuracy = np.mean(predicted_classes[class_mask] == i)
        else:
            class_accuracy = 0.0
        class_accuracies.append(class_accuracy)
    
    # Create results dictionary
    results = {
        "timestamp": datetime.now().isoformat(),
        "overall_accuracy": float(overall_accuracy),
        "total_test_images": int(val_generator.samples),
        "classes": []
    }
    
    # Clean up class names for display
    def clean_class_name(name):
        """Convert technical class name to readable format"""
        name = name.replace("Tomato___", "").replace("_", " ")
        # Special cases
        if "Two-spotted spider mite" in name or "Spider mites" in name:
            return "Spider Mites"
        if "Yellow Leaf Curl Virus" in name:
            return "Yellow Leaf Curl Virus"
        if "mosaic virus" in name:
            return "Mosaic Virus"
        if "healthy" in name.lower():
            return "Healthy"
        return name.title()
    
    # Print Table 16
    print("\n" + "=" * 80)
    print("TABLE 16: PERFORMANCE EVALUATION OF THE CLASSIFICATION")
    print("=" * 80)
    print(f"{'Tomato Disease':<35} {'Test Images':<15} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
    print("-" * 80)
    
    for i, class_name in enumerate(class_names):
        clean_name = clean_class_name(class_name)
        num_test = int(support[i])
        acc = class_accuracies[i]
        prec = precision[i]
        rec = recall[i]
        f1_score = f1[i]
        
        # Store in results
        results["classes"].append({
            "name": clean_name,
            "technical_name": class_name,
            "test_images": num_test,
            "accuracy": float(acc),
            "precision": float(prec),
            "recall": float(rec),
            "f1_score": float(f1_score)
        })
        
        # Print row
        print(f"{clean_name:<35} {num_test:<15} {acc:<12.2f} {prec:<12.2f} {rec:<12.2f} {f1_score:<12.2f}")
    
    print("-" * 80)
    print(f"{'TOTAL:':<35} {val_generator.samples:<15} {'HIGH':<12} {'HIGH':<12} {'HIGH':<12} {'HIGH':<12}")
    print(f"{'Overall Accuracy:':<35} {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
    print("=" * 80)
    
    # Save results to JSON
    output_file = "performance_evaluation_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n‚úÖ Results saved to: {output_file}")
    
    # Generate markdown table for thesis
    print("\n" + "=" * 80)
    print("üìù MARKDOWN TABLE FOR THESIS (Copy this to your document)")
    print("=" * 80)
    
    print("\n### Table 16. Performance Evaluation of the Classification\n")
    print("| Tomato Disease | Number of Test Images | Accuracy | Precision | Recall | F1-Score |")
    print("|----------------|----------------------|----------|-----------|--------|----------|")
    
    for i, class_name in enumerate(class_names):
        clean_name = clean_class_name(class_name)
        num_test = int(support[i])
        acc = class_accuracies[i]
        prec = precision[i]
        rec = recall[i]
        f1_score = f1[i]
        
        print(f"| {clean_name:<14} | {num_test:<20} | {acc:.2f} | {prec:.2f} | {rec:.2f} | {f1_score:.2f} |")
    
    print(f"| **TOTAL** | **{val_generator.samples}** | **HIGH** | **HIGH** | **HIGH** | **HIGH** |")
    print(f"\n**Overall Accuracy:** {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\n")
    
    # Generate confusion matrix summary
    print("\n" + "=" * 80)
    print("üìä CONFUSION MATRIX SUMMARY")
    print("=" * 80)
    
    cm = confusion_matrix(true_classes, predicted_classes)
    
    print("\nMost Common Misclassifications:")
    print("-" * 50)
    
    misclassifications = []
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            if i != j and cm[i, j] > 0:
                misclassifications.append((
                    clean_class_name(class_names[i]),
                    clean_class_name(class_names[j]),
                    cm[i, j]
                ))
    
    # Sort by count
    misclassifications.sort(key=lambda x: x[2], reverse=True)
    
    for true_class, pred_class, count in misclassifications[:10]:
        print(f"{true_class:<25} ‚Üí {pred_class:<25} ({count} times)")
    
    # Performance interpretation
    print("\n" + "=" * 80)
    print("üìà PERFORMANCE INTERPRETATION")
    print("=" * 80)
    
    if overall_accuracy >= 0.90:
        rating = "EXCELLENT"
        interpretation = "The model demonstrates excellent performance suitable for production deployment."
    elif overall_accuracy >= 0.85:
        rating = "VERY GOOD"
        interpretation = "The model shows very good performance suitable for practical use with confidence thresholds."
    elif overall_accuracy >= 0.80:
        rating = "GOOD"
        interpretation = "The model shows good performance but may benefit from additional training data."
    else:
        rating = "MODERATE"
        interpretation = "The model shows moderate performance and should be improved before deployment."
    
    print(f"\nOverall Rating: {rating}")
    print(f"Interpretation: {interpretation}")
    
    # Identify best and worst performing classes
    best_idx = np.argmax(class_accuracies)
    worst_idx = np.argmin(class_accuracies)
    
    print(f"\n‚úÖ Best Performing Class: {clean_class_name(class_names[best_idx])} ({class_accuracies[best_idx]:.2%})")
    print(f"‚ö†Ô∏è  Worst Performing Class: {clean_class_name(class_names[worst_idx])} ({class_accuracies[worst_idx]:.2%})")
    
    return results

if __name__ == "__main__":
    try:
        results = generate_performance_table()
        
        print("\n" + "=" * 80)
        print("‚úÖ PERFORMANCE EVALUATION COMPLETE")
        print("=" * 80)
        print("\nNext Steps:")
        print("1. Copy the markdown table above to your thesis document")
        print("2. Review the performance_evaluation_results.json file")
        print("3. Use the confusion matrix data for discussion section")
        print("4. Include performance interpretation in your analysis")
        
    except Exception as e:
        print(f"\n‚ùå Error during evaluation: {e}")
        import traceback
        traceback.print_exc()
        print("\nTroubleshooting:")
        print("1. Ensure the validation dataset path is correct")
        print("2. Check that the model file exists")
        print("3. Verify TensorFlow is installed correctly")
