{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183b36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EFFICIENTNETB0 TWO-STAGE TRAINING\n",
      "================================================================================\n",
      "\n",
      "Loading libraries\n",
      "TensorFlow version: 2.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\altai\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EFFICIENTNETB0 TWO-STAGE TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nLoading libraries\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881674c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Training Configuration:\n",
      "   • Model: EfficientNetB0 (Two-Stage Training)\n",
      "   • Image Size: 224x224\n",
      "   • Batch Size: 16\n",
      "   • Stage 1: 20 epochs (custom layers only, LR=0.001)\n",
      "   • Stage 2: 30 epochs (fine-tune all, LR=0.0001)\n",
      "   • Total Epochs: 50\n",
      "\n",
      "Dataset paths verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# CONFIGURATION\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = r\"C:\\Users\\altai\\Desktop\\DATASET\\tomato leaf diseases dataset(augmented)\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"training\")\n",
    "VAL_PATH = os.path.join(DATASET_PATH, \"validation\")\n",
    "MODEL_SAVE_PATH = r\"C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\"\n",
    "HISTORY_PATH = r\"C:\\Users\\altai\\Desktop\\TLDI_system\\training_history_efficientnet_FIXED.json\"\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # Increased for better gradient estimates\n",
    "STAGE1_EPOCHS = 20  # Train custom layers only\n",
    "STAGE2_EPOCHS = 30  # Fine-tune entire model\n",
    "STAGE1_LR = 0.001  # Higher LR for custom layers\n",
    "STAGE2_LR = 0.0001  # Lower LR for fine-tuning\n",
    "\n",
    "print(f\"\"\"\n",
    "Training Configuration:\n",
    "   • Model: EfficientNetB0 (Two-Stage Training)\n",
    "   • Image Size: {IMG_SIZE}x{IMG_SIZE}\n",
    "   • Batch Size: {BATCH_SIZE}\n",
    "   • Stage 1: {STAGE1_EPOCHS} epochs (custom layers only, LR={STAGE1_LR})\n",
    "   • Stage 2: {STAGE2_EPOCHS} epochs (fine-tune all, LR={STAGE2_LR})\n",
    "   • Total Epochs: {STAGE1_EPOCHS + STAGE2_EPOCHS}\n",
    "\"\"\")\n",
    "\n",
    "# Verify paths\n",
    "if not os.path.exists(TRAIN_PATH):\n",
    "    print(f\"ERROR: Training folder not found at: {TRAIN_PATH}\")\n",
    "    exit(1)\n",
    "if not os.path.exists(VAL_PATH):\n",
    "    print(f\"ERROR: Validation folder not found at: {VAL_PATH}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Dataset paths verified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fea02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: DATA PREPARATION\n",
      "================================================================================\n",
      "Loading training data...\n",
      "Found 20452 images belonging to 11 classes.\n",
      "Loading validation data...\n",
      "Found 5488 images belonging to 11 classes.\n",
      "\n",
      "Data loaded:\n",
      "   • Training samples: 20452\n",
      "   • Validation samples: 5488\n",
      "   • Classes: 11\n",
      "   • Class names: ['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'Unidentified']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# DATA PREPARATION\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: DATA PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Training data with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data (no augmentation)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "Data loaded:\n",
    "   • Training samples: {train_generator.samples}\n",
    "   • Validation samples: {val_generator.samples}\n",
    "   • Classes: {train_generator.num_classes}\n",
    "   • Class names: {list(train_generator.class_indices.keys())}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570b4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: MODEL BUILDING\n",
      "================================================================================\n",
      "WARNING:tensorflow:From C:\\Users\\altai\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Building EfficientNetB0 model...\n",
      "EfficientNetB0 architecture created with 237 layers\n",
      "Loading ImageNet weights...\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2us/step\n",
      "ImageNet weights loaded successfully!\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB0_TomatoDisease\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"EfficientNetB0_TomatoDisease\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,827</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn2 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense2 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn3 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │         \u001b[38;5;34m2,827\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,847,790</span> (18.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,847,790\u001b[0m (18.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">794,123</span> (3.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m794,123\u001b[0m (3.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,053,667</span> (15.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,053,667\u001b[0m (15.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model built successfully!\n",
      "   • Total parameters: 4,847,790\n",
      "   • Trainable parameters: 794,123\n",
      "   • Frozen parameters: 4,053,667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# MODEL BUILDING\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: MODEL BUILDING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Building EfficientNetB0 model...\")\n",
    "\n",
    "# Create input\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Load base model without weights first\n",
    "base_model = EfficientNetB0(\n",
    "    weights=None,\n",
    "    include_top=False,\n",
    "    input_tensor=inputs\n",
    ")\n",
    "\n",
    "print(f\"EfficientNetB0 architecture created with {len(base_model.layers)} layers\")\n",
    "\n",
    "# Load ImageNet weights\n",
    "print(\"Loading ImageNet weights...\")\n",
    "try:\n",
    "    weights_path = tf.keras.utils.get_file(\n",
    "        'efficientnetb0_notop.h5',\n",
    "        'https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5',\n",
    "        cache_subdir='models',\n",
    "        file_hash=None  # Skip hash to avoid corruption\n",
    "    )\n",
    "    base_model.load_weights(weights_path)\n",
    "    print(\"ImageNet weights loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load ImageNet weights: {e}\")\n",
    "    print(\"Continuing with random initialization\")\n",
    "\n",
    "# Freeze base model for Stage 1\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "x = BatchNormalization(name='bn1')(x)\n",
    "x = Dropout(0.5, name='dropout1')(x)  # Increased dropout\n",
    "x = Dense(512, activation='relu', name='dense1')(x)\n",
    "x = BatchNormalization(name='bn2')(x)\n",
    "x = Dropout(0.4, name='dropout2')(x)\n",
    "x = Dense(256, activation='relu', name='dense2')(x)\n",
    "x = BatchNormalization(name='bn3')(x)\n",
    "x = Dropout(0.3, name='dropout3')(x)\n",
    "outputs = Dense(train_generator.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs, outputs, name='EfficientNetB0_TomatoDisease')\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "total_params = model.count_params()\n",
    "print(f\"\"\"\n",
    "Model built successfully!\n",
    "   • Total parameters: {total_params:,}\n",
    "   • Trainable parameters: {trainable_params:,}\n",
    "   • Frozen parameters: {total_params - trainable_params:,}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3431d072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: TRAINING CUSTOM LAYERS (Base Frozen)\n",
      "================================================================================\n",
      "\n",
      "Starting Stage 1 Training:\n",
      "   • Epochs: 20\n",
      "   • Learning Rate: 0.001\n",
      "   • Base Model: FROZEN\n",
      "   • Training: Custom layers only\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1350 - loss: 2.7577\n",
      "Epoch 1: val_accuracy improved from None to 0.08710, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED_stage1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED_stage1.h5\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1805s\u001b[0m 1s/step - accuracy: 0.1865 - loss: 2.4465 - val_accuracy: 0.0871 - val_loss: 3.2043 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3234 - loss: 1.9264\n",
      "Epoch 2: val_accuracy improved from 0.08710 to 0.16454, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED_stage1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED_stage1.h5\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2402s\u001b[0m 2s/step - accuracy: 0.3520 - loss: 1.8408 - val_accuracy: 0.1645 - val_loss: 13.2985 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4550 - loss: 1.5519\n",
      "Epoch 3: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2899s\u001b[0m 2s/step - accuracy: 0.4835 - loss: 1.4671 - val_accuracy: 0.0909 - val_loss: 17.6712 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5741 - loss: 1.2215\n",
      "Epoch 4: val_accuracy improved from 0.16454 to 0.17238, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED_stage1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED_stage1.h5\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1914s\u001b[0m 1s/step - accuracy: 0.5882 - loss: 1.1764 - val_accuracy: 0.1724 - val_loss: 4.7175 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.6403 - loss: 1.0160\n",
      "Epoch 5: val_accuracy did not improve from 0.17238\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 504ms/step - accuracy: 0.6539 - loss: 0.9839 - val_accuracy: 0.1518 - val_loss: 7.7666 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - accuracy: 0.6772 - loss: 0.9214\n",
      "Epoch 6: val_accuracy did not improve from 0.17238\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 503ms/step - accuracy: 0.6821 - loss: 0.9027 - val_accuracy: 0.1226 - val_loss: 10.5785 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Stage 1 completed in 2:51:49.950171\n",
      "Best validation accuracy: 17.24%\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STAGE 1: TRAIN CUSTOM LAYERS ONLY\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STAGE 1: TRAINING CUSTOM LAYERS (Base Frozen)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compile for Stage 1\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=STAGE1_LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for Stage 1\n",
    "stage1_callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH.replace('.h5', '_stage1.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\"\"\n",
    "Starting Stage 1 Training:\n",
    "   • Epochs: {STAGE1_EPOCHS}\n",
    "   • Learning Rate: {STAGE1_LR}\n",
    "   • Base Model: FROZEN\n",
    "   • Training: Custom layers only\n",
    "\"\"\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "history_stage1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=STAGE1_EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=stage1_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stage1_time = datetime.now() - start_time\n",
    "print(f\"\\nStage 1 completed in {stage1_time}\")\n",
    "print(f\"Best validation accuracy: {max(history_stage1.history['val_accuracy'])*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9bc3d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 2: FINE-TUNING ENTIRE MODEL (Base Unfrozen)\n",
      "================================================================================\n",
      "✓ Base model unfrozen (first 100 layers still frozen)\n",
      "   • Trainable parameters: 4,634,471\n",
      "\n",
      "Starting Stage 2 Training:\n",
      "   • Epochs: 30\n",
      "   • Learning Rate: 0.0001\n",
      "   • Base Model: PARTIALLY UNFROZEN\n",
      "   • Training: Entire model (fine-tuning)\n",
      "\n",
      "Epoch 1/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - accuracy: 0.3280 - loss: 1.8902\n",
      "Epoch 1: val_accuracy improved from None to 0.09457, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 518ms/step - accuracy: 0.3528 - loss: 1.8288 - val_accuracy: 0.0946 - val_loss: 9.1534 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.4045 - loss: 1.6730\n",
      "Epoch 2: val_accuracy did not improve from 0.09457\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 510ms/step - accuracy: 0.4249 - loss: 1.6303 - val_accuracy: 0.0853 - val_loss: 13.6007 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.4989 - loss: 1.4399\n",
      "Epoch 3: val_accuracy improved from 0.09457 to 0.09585, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 508ms/step - accuracy: 0.5211 - loss: 1.3794 - val_accuracy: 0.0958 - val_loss: 19.2263 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step - accuracy: 0.5652 - loss: 1.2457\n",
      "Epoch 4: val_accuracy improved from 0.09585 to 0.10860, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 506ms/step - accuracy: 0.5815 - loss: 1.1979 - val_accuracy: 0.1086 - val_loss: 20.7177 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - accuracy: 0.6449 - loss: 1.0246\n",
      "Epoch 5: val_accuracy improved from 0.10860 to 0.10969, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 508ms/step - accuracy: 0.6569 - loss: 0.9916 - val_accuracy: 0.1097 - val_loss: 23.9693 - learning_rate: 5.0000e-05\n",
      "Epoch 6/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6854 - loss: 0.9195\n",
      "Epoch 6: val_accuracy did not improve from 0.10969\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7979s\u001b[0m 6s/step - accuracy: 0.6899 - loss: 0.9014 - val_accuracy: 0.0791 - val_loss: 45.0016 - learning_rate: 5.0000e-05\n",
      "Epoch 7/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.7057 - loss: 0.8553\n",
      "Epoch 7: val_accuracy improved from 0.10969 to 0.14158, saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: finished saving model to C:\\Users\\altai\\Desktop\\TLDI_system\\trained_model_efficientnet_FIXED.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 468ms/step - accuracy: 0.7080 - loss: 0.8498 - val_accuracy: 0.1416 - val_loss: 42.1117 - learning_rate: 5.0000e-05\n",
      "Epoch 8/30\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.7344 - loss: 0.7785\n",
      "Epoch 8: val_accuracy did not improve from 0.14158\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 466ms/step - accuracy: 0.7346 - loss: 0.7720 - val_accuracy: 0.1091 - val_loss: 48.1775 - learning_rate: 2.5000e-05\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Stage 2 completed in 3:27:33.468287\n",
      "Best validation accuracy: 14.16%\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STAGE 2: FINE-TUNE ENTIRE MODEL\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STAGE 2: FINE-TUNING ENTIRE MODEL (Base Unfrozen)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Unfreeze base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze first 100 layers (keep low-level features frozen)\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"✓ Base model unfrozen (first 100 layers still frozen)\")\n",
    "print(f\"   • Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=STAGE2_LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for Stage 2\n",
    "stage2_callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\"\"\n",
    "Starting Stage 2 Training:\n",
    "   • Epochs: {STAGE2_EPOCHS}\n",
    "   • Learning Rate: {STAGE2_LR}\n",
    "   • Base Model: PARTIALLY UNFROZEN\n",
    "   • Training: Entire model (fine-tuning)\n",
    "\"\"\")\n",
    "\n",
    "stage2_start = datetime.now()\n",
    "\n",
    "history_stage2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=STAGE2_EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=stage2_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stage2_time = datetime.now() - stage2_start\n",
    "total_time = datetime.now() - start_time\n",
    "\n",
    "print(f\"\\nStage 2 completed in {stage2_time}\")\n",
    "print(f\"Best validation accuracy: {max(history_stage2.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29145b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION\n",
      "================================================================================\n",
      "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 162ms/step - accuracy: 0.0946 - loss: 9.1534\n",
      "\n",
      "📊 FINAL RESULTS:\n",
      "   ✅ Validation Loss: 9.1534\n",
      "   ✅ Validation Accuracy: 9.46%\n",
      "\n",
      "   Stage 1 Best: 17.24%\n",
      "   Stage 2 Best: 14.16%\n",
      "\n",
      "   Total Training Time: 6:23:01.622711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# FINAL EVALUATION\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 FINAL RESULTS:\n",
    "   ✅ Validation Loss: {val_loss:.4f}\n",
    "   ✅ Validation Accuracy: {val_accuracy*100:.2f}%\n",
    "   \n",
    "   Stage 1 Best: {max(history_stage1.history['val_accuracy'])*100:.2f}%\n",
    "   Stage 2 Best: {max(history_stage2.history['val_accuracy'])*100:.2f}%\n",
    "   \n",
    "   Total Training Time: {total_time}\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
