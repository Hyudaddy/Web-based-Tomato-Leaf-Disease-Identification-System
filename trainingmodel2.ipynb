{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9ff429-f158-4565-84a1-80366fcb929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FITO - EfficientNetB0 LOCAL TRAINING\n",
      "Optimized for Intel Core i3-6006U with 12GB RAM\n",
      "================================================================================\n",
      "\n",
      "Loading libraries (this may take a moment)...\n",
      "✓ All libraries loaded successfully!\n",
      "TensorFlow version: 2.20.0\n",
      "No dedicated GPU - using CPU for training\n",
      "This will take longer but will work fine!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "================================================================================\n",
    "TOMATO LEAF DISEASE MODEL\n",
    "================================================================================\n",
    "Local Training Version with EfficientNetB0 (224x224) for Higher Accuracy\n",
    "\n",
    "OPTIMIZED FOR:\n",
    "--------------\n",
    "- Intel Core i3-6006U @ 2.00GHz (2 cores, 4 threads)\n",
    "- 12 GB RAM\n",
    "- CPU-only training (no dedicated GPU)\n",
    "\n",
    "ESTIMATED TRAINING TIME: 4-6 hours (50 epochs)\n",
    "\n",
    "TRAINING PROCESS FLOW:\n",
    "----------------------\n",
    "1. CONFIGURATION   - Set hyperparameters (epochs, batch size, learning rate)\n",
    "2. DATA PREPARATION- Load and augment training/validation images\n",
    "3. MODEL BUILDING  - Create neural network using transfer learning (EfficientNetB0)\n",
    "4. TRAINING        - Train the model on the dataset\n",
    "5. VALIDATION      - Evaluate model performance on unseen data\n",
    "6. SAVING          - Save trained model and training history\n",
    "\n",
    "HOW TO RUN:\n",
    "-----------\n",
    "1. Open terminal in the TLDI_system folder\n",
    "2. Run: python train_model_local_efficientnet.py\n",
    "3. Wait for training to complete (4-6 hours)\n",
    "4. The new model will be saved as: trained_model_efficientnet.h5\n",
    "\"\"\"\n",
    "\n",
    "# ================================================================================\n",
    "# IMPORTS - Required libraries for deep learning and data processing\n",
    "# ================================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"FITO - EfficientNetB0 LOCAL TRAINING\")\n",
    "print(\"Optimized for Intel Core i3-6006U with 12GB RAM\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nLoading libraries (this may take a moment)...\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"✓ All libraries loaded successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check for GPU (you have integrated Intel HD Graphics - won't help much)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU detected: {gpus}\")\n",
    "else:\n",
    "    print(\"No dedicated GPU - using CPU for training\")\n",
    "    print(\"This will take longer but will work fine!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d3a829-7c97-4733-8bce-8f9793893eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: CONFIGURATION - Setting Hyperparameters\n",
      "================================================================================\n",
      "\n",
      "Training Configuration (EfficientNetB0 for i3 CPU):\n",
      "   • Model: EfficientNetB0 (pre-trained on ImageNet)\n",
      "   • Image Size: 224x224 pixels (higher detail)\n",
      "   • Batch Size: 8 images per step (optimized for 12GB RAM)\n",
      "   • Epochs: 50 training cycles\n",
      "   • Learning Rate: 0.001\n",
      "   • Device: Intel Core i3-6006U CPU\n",
      "\n",
      "Dataset Paths:\n",
      "   • Training: C:\\Users\\HYUDADDY\\Desktop\\DATASET\\tomato leaf diseases dataset(augmented)\\training\n",
      "   • Validation: C:\\Users\\HYUDADDY\\Desktop\\DATASET\\tomato leaf diseases dataset(augmented)\\validation\n",
      "\n",
      "Output:\n",
      "   • Model will be saved to: C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\n",
      "\n",
      "Estimated Training Time: 4-6 hours\n",
      "   (You can leave this running in the background)\n",
      "\n",
      "Training folder found!\n",
      "Validation folder found!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 1: CONFIGURATION - Define paths and hyperparameters\n",
    "# ================================================================================\n",
    "# This is where we configure all the settings BEFORE training begins.\n",
    "# The hyperparameters are OPTIMIZED for your Intel i3 processor.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: CONFIGURATION - Setting Hyperparameters\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dataset paths - YOUR LOCAL PATHS\n",
    "DATASET_PATH = r\"C:\\Users\\HYUDADDY\\Desktop\\DATASET\\tomato leaf diseases dataset(augmented)\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"training\")      # Training images\n",
    "VAL_PATH = os.path.join(DATASET_PATH, \"validation\")      # Validation images\n",
    "\n",
    "# Output paths - Where to save the trained model\n",
    "MODEL_SAVE_PATH = r\"C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\"\n",
    "BACKUP_MODEL_PATH = r\"C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet_backup.h5\"\n",
    "HISTORY_PATH = r\"C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\training_history_efficientnet.json\"\n",
    "\n",
    "# HYPERPARAMETERS - OPTIMIZED FOR YOUR i3 PROCESSOR + 12GB RAM\n",
    "# ==============================================================================\n",
    "IMG_SIZE = 224          # Image resolution: 224x224 (optimal for EfficientNet)\n",
    "BATCH_SIZE = 8          # REDUCED from 32 to fit in your RAM (i3 optimization)\n",
    "EPOCHS = 50             # Number of training cycles\n",
    "LEARNING_RATE = 0.001   # How fast the model adjusts weights\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\"\"\n",
    "Training Configuration (EfficientNetB0 for i3 CPU):\n",
    "   • Model: EfficientNetB0 (pre-trained on ImageNet)\n",
    "   • Image Size: {IMG_SIZE}x{IMG_SIZE} pixels (higher detail)\n",
    "   • Batch Size: {BATCH_SIZE} images per step (optimized for 12GB RAM)\n",
    "   • Epochs: {EPOCHS} training cycles\n",
    "   • Learning Rate: {LEARNING_RATE}\n",
    "   • Device: Intel Core i3-6006U CPU\n",
    "   \n",
    "Dataset Paths:\n",
    "   • Training: {TRAIN_PATH}\n",
    "   • Validation: {VAL_PATH}\n",
    "   \n",
    "Output:\n",
    "   • Model will be saved to: {MODEL_SAVE_PATH}\n",
    "   \n",
    "Estimated Training Time: 4-6 hours\n",
    "   (You can leave this running in the background)\n",
    "\"\"\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(TRAIN_PATH):\n",
    "    print(f\"ERROR: Training folder not found at: {TRAIN_PATH}\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"Training folder found!\")\n",
    "    \n",
    "if not os.path.exists(VAL_PATH):\n",
    "    print(f\"ERROR: Validation folder not found at: {VAL_PATH}\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"Validation folder found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc89a93-da89-4d0b-8f80-1031868eda20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: DATA PREPARATION - Loading and Augmenting Images\n",
      "================================================================================\n",
      "Loading training data (this may take a minute)...\n",
      "Found 20452 images belonging to 11 classes.\n",
      "Training data loaded: 20452 images\n",
      "Loading validation data...\n",
      "Found 5488 images belonging to 11 classes.\n",
      "Validation data loaded: 5488 images\n",
      "\n",
      "Dataset Summary:\n",
      "   • Training samples: 20452\n",
      "   • Validation samples: 5488\n",
      "   • Number of classes: 11\n",
      "   • Classes: ['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'Unidentified']\n",
      "   • Steps per epoch: 2556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# STEP 2: DATA PREPARATION - Configure how images are loaded and augmented\n",
    "# ================================================================================\n",
    "# Data augmentation artificially increases dataset variety by creating modified\n",
    "# versions of images (rotated, flipped, brightness adjusted). This helps the\n",
    "# model generalize better to real-world images with different lighting/angles.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: DATA PREPARATION - Loading and Augmenting Images\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# TRAINING DATA AUGMENTATION - Apply transformations to increase variety\n",
    "# These transformations create variations of your images to help the model\n",
    "# learn to recognize diseases from different angles, lighting conditions, etc.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,             # Normalize pixel values from 0-255 to 0-1\n",
    "    rotation_range=30,          # Rotate images randomly up to 30 degrees\n",
    "    width_shift_range=0.2,      # Shift image horizontally by up to 20%\n",
    "    height_shift_range=0.2,     # Shift image vertically by up to 20%\n",
    "    shear_range=0.2,            # Apply shearing transformation\n",
    "    zoom_range=0.2,             # Zoom in/out by up to 20%\n",
    "    brightness_range=[0.8, 1.2],  # Adjust brightness (for poor lighting)\n",
    "    horizontal_flip=True,       # Flip images horizontally\n",
    "    vertical_flip=True,         # Flip images vertically\n",
    "    fill_mode='nearest'         # How to fill empty pixels after transforms\n",
    ")\n",
    "\n",
    "# VALIDATION DATA - Only normalize, NO augmentation\n",
    "# Validation data should represent real-world images without modifications\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# LOAD TRAINING DATA - Read images from the training folder\n",
    "# The generator reads images from folders, where each folder name = class label\n",
    "print(\"Loading training data (this may take a minute)...\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,                         # Path to training images folder\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),   # Resize all images to 224x224\n",
    "    batch_size=BATCH_SIZE,              # Load 8 images at a time (RAM optimized)\n",
    "    class_mode='categorical',           # Multi-class classification (one-hot)\n",
    "    shuffle=True                        # Randomize order each epoch\n",
    ")\n",
    "print(f\"Training data loaded: {train_generator.samples} images\")\n",
    "\n",
    "# LOAD VALIDATION DATA - Used to check model performance during training\n",
    "print(\"Loading validation data...\")\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_PATH,                           # Path to validation images folder\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),   # Same size as training\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False                       # Don't shuffle for consistent evaluation\n",
    ")\n",
    "print(f\"Validation data loaded: {val_generator.samples} images\")\n",
    "\n",
    "# Display dataset summary\n",
    "print(f\"\"\"\n",
    "Dataset Summary:\n",
    "   • Training samples: {train_generator.samples}\n",
    "   • Validation samples: {val_generator.samples}\n",
    "   • Number of classes: {train_generator.num_classes}\n",
    "   • Classes: {list(train_generator.class_indices.keys())}\n",
    "   • Steps per epoch: {train_generator.samples // BATCH_SIZE}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "333a3421-5cd5-4ab5-b38c-61ea0fd98786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 3: MODEL BUILDING - Creating EfficientNetB0 Architecture\n",
      "================================================================================\n",
      "Building EfficientNetB0 model...\n",
      "EfficientNetB0 architecture created with 237 layers\n",
      "Downloading and loading ImageNet weights...\n",
      "ImageNet weights loaded successfully!\n",
      "Compiling model...\n",
      "\n",
      "Model Architecture Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,827</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │         \u001b[38;5;34m2,827\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,846,766</span> (18.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,846,766\u001b[0m (18.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">793,611</span> (3.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m793,611\u001b[0m (3.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,053,155</span> (15.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,053,155\u001b[0m (15.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model built successfully!\n",
      "   - Base model: EfficientNetB0 (frozen)\n",
      "   - Custom layers: GlobalPooling -> Dense(512) -> Dense(256) -> Output(11)\n",
      "   - Total parameters: 4,846,766\n",
      "   - Trainable parameters: 793,611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 3: MODEL BUILDING - Creating EfficientNetB0 Architecture\n",
    "# ================================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: MODEL BUILDING - Creating EfficientNetB0 Architecture\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# STEP 3A: Build model using transfer learning\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Building EfficientNetB0 model...\")\n",
    "\n",
    "# Create the input tensor explicitly first\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Load base model structure WITHOUT weights to avoid shape mismatch\n",
    "base_model = EfficientNetB0(\n",
    "    weights=None,  # Do not load weights during instantiation\n",
    "    include_top=False,\n",
    "    input_tensor=inputs\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "print(f\"EfficientNetB0 architecture created with {len(base_model.layers)} layers\")\n",
    "\n",
    "# Manually load ImageNet weights (without hash verification to avoid corruption issues)\n",
    "print(\"Downloading and loading ImageNet weights...\")\n",
    "try:\n",
    "    weights_path = tf.keras.utils.get_file(\n",
    "        'efficientnetb0_notop.h5',\n",
    "        'https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5',\n",
    "        cache_subdir='models',\n",
    "        file_hash=None  # Skip hash verification to avoid corruption errors\n",
    "    )\n",
    "    base_model.load_weights(weights_path)\n",
    "    print(\"ImageNet weights loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load ImageNet weights: {e}\")\n",
    "    print(\"Continuing with random initialization (accuracy will be lower)\")\n",
    "\n",
    "# STEP 3B: Add custom classification layers\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# STEP 3C: Compile model\n",
    "print(\"Compiling model...\")\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "model.summary()\n",
    "\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"\"\"\n",
    "Model built successfully!\n",
    "   - Base model: EfficientNetB0 (frozen)\n",
    "   - Custom layers: GlobalPooling -> Dense(512) -> Dense(256) -> Output({train_generator.num_classes})\n",
    "   - Total parameters: {total_params:,}\n",
    "   - Trainable parameters: {trainable_params:,}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7d0680-5831-449a-8232-f12fcc6c19ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING - Starting Model Training\n",
      "================================================================================\n",
      "\n",
      "Starting Training:\n",
      "   • Epochs: 50\n",
      "   • Batch size: 8\n",
      "   • Steps per epoch: 2556\n",
      "   • Validation steps: 686\n",
      "   • Early stopping patience: 7 epochs\n",
      "\n",
      "Estimated time: ~127 minutes per epoch\n",
      "   Total: approximately 105 hours\n",
      "\n",
      "TIP: You can minimize this window and let it run in background.\n",
      "   The best model is automatically saved whenever accuracy improves.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING STARTED - Please wait...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - accuracy: 0.0896 - loss: 2.9240\n",
      "Epoch 1: val_accuracy improved from None to 0.08746, saving model to C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1476s\u001b[0m 569ms/step - accuracy: 0.0903 - loss: 2.6712 - val_accuracy: 0.0875 - val_loss: 2.3980 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.0919 - loss: 2.4181\n",
      "Epoch 2: val_accuracy improved from 0.08746 to 0.16436, saving model to C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1485s\u001b[0m 578ms/step - accuracy: 0.0933 - loss: 2.4123 - val_accuracy: 0.1644 - val_loss: 2.3923 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - accuracy: 0.0998 - loss: 2.4022\n",
      "Epoch 3: val_accuracy improved from 0.16436 to 0.16454, saving model to C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1243s\u001b[0m 468ms/step - accuracy: 0.1012 - loss: 2.4010 - val_accuracy: 0.1645 - val_loss: 2.3887 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.1004 - loss: 2.3995\n",
      "Epoch 4: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1269s\u001b[0m 496ms/step - accuracy: 0.1018 - loss: 2.3988 - val_accuracy: 0.1645 - val_loss: 2.3887 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - accuracy: 0.1026 - loss: 2.3972\n",
      "Epoch 5: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1250s\u001b[0m 489ms/step - accuracy: 0.1024 - loss: 2.3974 - val_accuracy: 0.1645 - val_loss: 2.3877 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - accuracy: 0.1044 - loss: 2.3966\n",
      "Epoch 6: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1242s\u001b[0m 486ms/step - accuracy: 0.1027 - loss: 2.3971 - val_accuracy: 0.1645 - val_loss: 2.3880 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - accuracy: 0.1032 - loss: 2.3970\n",
      "Epoch 7: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1246s\u001b[0m 487ms/step - accuracy: 0.1027 - loss: 2.3970 - val_accuracy: 0.1645 - val_loss: 2.3889 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.1040 - loss: 2.3961\n",
      "Epoch 8: val_accuracy did not improve from 0.16454\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1300s\u001b[0m 494ms/step - accuracy: 0.1030 - loss: 2.3965 - val_accuracy: 0.1645 - val_loss: 2.3886 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520ms/step - accuracy: 0.1081 - loss: 2.3951\n",
      "Epoch 9: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1592s\u001b[0m 623ms/step - accuracy: 0.1030 - loss: 2.3964 - val_accuracy: 0.1645 - val_loss: 2.3885 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520ms/step - accuracy: 0.1037 - loss: 2.3963\n",
      "Epoch 10: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1599s\u001b[0m 606ms/step - accuracy: 0.1030 - loss: 2.3964 - val_accuracy: 0.1645 - val_loss: 2.3877 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - accuracy: 0.0993 - loss: 2.3969\n",
      "Epoch 11: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1450s\u001b[0m 567ms/step - accuracy: 0.1031 - loss: 2.3964 - val_accuracy: 0.1645 - val_loss: 2.3872 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - accuracy: 0.1044 - loss: 2.3963\n",
      "Epoch 12: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1262s\u001b[0m 494ms/step - accuracy: 0.1030 - loss: 2.3964 - val_accuracy: 0.1645 - val_loss: 2.3879 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - accuracy: 0.1009 - loss: 2.3969\n",
      "Epoch 13: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1263s\u001b[0m 494ms/step - accuracy: 0.1030 - loss: 2.3964 - val_accuracy: 0.1645 - val_loss: 2.3871 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - accuracy: 0.0972 - loss: 2.3964\n",
      "Epoch 14: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1286s\u001b[0m 503ms/step - accuracy: 0.1030 - loss: 2.3963 - val_accuracy: 0.1645 - val_loss: 2.3867 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.1023 - loss: 2.3960\n",
      "Epoch 15: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1292s\u001b[0m 505ms/step - accuracy: 0.1030 - loss: 2.3963 - val_accuracy: 0.1645 - val_loss: 2.3875 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - accuracy: 0.1026 - loss: 2.3959\n",
      "Epoch 16: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1289s\u001b[0m 504ms/step - accuracy: 0.1030 - loss: 2.3963 - val_accuracy: 0.1645 - val_loss: 2.3872 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.1042 - loss: 2.3959\n",
      "Epoch 17: val_accuracy did not improve from 0.16454\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 506ms/step - accuracy: 0.1030 - loss: 2.3963 - val_accuracy: 0.1645 - val_loss: 2.3877 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.1046 - loss: 2.3960\n",
      "Epoch 18: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 506ms/step - accuracy: 0.1030 - loss: 2.3962 - val_accuracy: 0.1645 - val_loss: 2.3878 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.1041 - loss: 2.3959\n",
      "Epoch 19: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1296s\u001b[0m 507ms/step - accuracy: 0.1030 - loss: 2.3962 - val_accuracy: 0.1645 - val_loss: 2.3877 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - accuracy: 0.0992 - loss: 2.3971\n",
      "Epoch 20: val_accuracy did not improve from 0.16454\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1325s\u001b[0m 518ms/step - accuracy: 0.1030 - loss: 2.3962 - val_accuracy: 0.1645 - val_loss: 2.3875 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513ms/step - accuracy: 0.1029 - loss: 2.3965\n",
      "Epoch 21: val_accuracy did not improve from 0.16454\n",
      "\u001b[1m2557/2557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1564s\u001b[0m 612ms/step - accuracy: 0.1030 - loss: 2.3962 - val_accuracy: 0.1645 - val_loss: 2.3875 - learning_rate: 1.2500e-04\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETED!\n",
      "================================================================================\n",
      "Total training time: 7:51:57.429822\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 4: TRAINING - Train the model on the dataset\n",
    "# ================================================================================\n",
    "# This is the main training loop where the model learns from the images.\n",
    "# For each epoch, the model sees ALL training images and adjusts its weights.\n",
    "#\n",
    "# TRAINING METRICS:\n",
    "# - loss: How wrong the model's predictions are (lower is better)\n",
    "# - accuracy: Percentage of correct predictions (higher is better)\n",
    "# - val_loss: Loss on validation data (monitors overfitting)\n",
    "# - val_accuracy: Accuracy on validation data (actual performance)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: TRAINING - Starting Model Training\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# CALLBACKS - Special functions that run during training\n",
    "callbacks = [\n",
    "    # ModelCheckpoint: Save the model whenever validation accuracy improves\n",
    "    ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        monitor='val_accuracy',    # Watch validation accuracy\n",
    "        save_best_only=True,       # Only save if it's the best so far\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    # EarlyStopping: Stop training if model stops improving\n",
    "    # This prevents overfitting by stopping when validation loss plateaus\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,                # Stop if no improvement for 7 epochs\n",
    "        restore_best_weights=True, # Go back to the best weights\n",
    "        verbose=1\n",
    "    ),\n",
    "    # ReduceLROnPlateau: Lower learning rate if training plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,                # Reduce LR by half\n",
    "        patience=3,                # Wait 3 epochs before reducing\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = val_generator.samples // BATCH_SIZE\n",
    "estimated_time_per_epoch = (steps_per_epoch * 3) // 60  # rough estimate: ~3 seconds per step\n",
    "\n",
    "print(f\"\"\"\n",
    "Starting Training:\n",
    "   • Epochs: {EPOCHS}\n",
    "   • Batch size: {BATCH_SIZE}\n",
    "   • Steps per epoch: {steps_per_epoch}\n",
    "   • Validation steps: {validation_steps}\n",
    "   • Early stopping patience: 7 epochs\n",
    "   \n",
    "Estimated time: ~{estimated_time_per_epoch} minutes per epoch\n",
    "   Total: approximately {estimated_time_per_epoch * EPOCHS // 60} hours\n",
    "   \n",
    "TIP: You can minimize this window and let it run in background.\n",
    "   The best model is automatically saved whenever accuracy improves.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TRAINING STARTED - Please wait...\")\n",
    "print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# *** THIS IS WHERE THE ACTUAL TRAINING HAPPENS ***\n",
    "start_time = datetime.now()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,              # Training data\n",
    "    epochs=EPOCHS,                # Train for 50 epochs (or until early stop)\n",
    "    validation_data=val_generator,  # Validation data to monitor overfitting\n",
    "    callbacks=callbacks,          # Run checkpoint, early stop, LR reduction\n",
    "    verbose=1                     # Show progress bar\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total training time: {training_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c413a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: VALIDATION - Evaluating Model Performance\n",
      "================================================================================\n",
      "\u001b[1m686/686\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 378ms/step - accuracy: 0.1645 - loss: 2.3867\n",
      "\n",
      "FINAL RESULTS:\n",
      "   Validation Loss: 2.3867\n",
      "   Validation Accuracy: 0.1645 (16.45%)\n",
      "\n",
      "   Epochs trained: 21\n",
      "   Best validation accuracy: 16.45%\n",
      "   Training time: 7:51:57.429822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 5: VALIDATION/EVALUATION - Evaluate the trained model\n",
    "# ================================================================================\n",
    "# After training, we evaluate the model on the validation set one final time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: VALIDATION - Evaluating Model Performance\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "\n",
    "print(f\"\"\"\n",
    "FINAL RESULTS:\n",
    "   Validation Loss: {val_loss:.4f}\n",
    "   Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\n",
    "   \n",
    "   Epochs trained: {len(history.history['accuracy'])}\n",
    "   Best validation accuracy: {max(history.history['val_accuracy'])*100:.2f}%\n",
    "   Training time: {training_duration}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7acdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: SAVING - Saving Training History\n",
      "================================================================================\n",
      "Training history saved to: C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\training_history_efficientnet.json\n",
      "Model saved to: C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 6: SAVE TRAINING HISTORY - Save metrics for documentation\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: SAVING - Saving Training History\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save training history as JSON\n",
    "history_dict = {\n",
    "    'accuracy': [float(x) for x in history.history['accuracy']],\n",
    "    'loss': [float(x) for x in history.history['loss']],\n",
    "    'val_accuracy': [float(x) for x in history.history['val_accuracy']],\n",
    "    'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "    'final_val_accuracy': float(val_accuracy),\n",
    "    'final_val_loss': float(val_loss),\n",
    "    'training_duration': str(training_duration),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'model': 'EfficientNetB0',\n",
    "        'img_size': IMG_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'epochs_trained': len(history.history['accuracy']),\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'training_samples': train_generator.samples,\n",
    "        'validation_samples': val_generator.samples,\n",
    "        'num_classes': train_generator.num_classes,\n",
    "        'classes': list(train_generator.class_indices.keys())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(HISTORY_PATH, 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "    \n",
    "print(f\"Training history saved to: {HISTORY_PATH}\")\n",
    "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "301b1cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "TRAINING SUMMARY:\n",
      "   • Model: EfficientNetB0 (Transfer Learning from ImageNet)\n",
      "   • Image Size: 224x224\n",
      "   • Final Accuracy: 16.45%\n",
      "   • Best Accuracy: 16.45%\n",
      "   • Epochs Trained: 21 / 50\n",
      "   • Training Time: 7:51:57.429822\n",
      "\n",
      "SAVED FILES:\n",
      "   1. C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\n",
      "      - The trained model file\n",
      "   2. C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\training_history_efficientnet.json\n",
      "      - Training metrics in JSON format\n",
      "\n",
      "TO USE THE NEW MODEL:\n",
      "   1. Copy the new model to the backend folder:\n",
      "\n",
      "      copy \"C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\trained_model_efficientnet.h5\" \"C:\\Users\\HYUDADDY\\Desktop\\TLDI_system\\backend\\trained_model_fito.h5\"\n",
      "\n",
      "   2. IMPORTANT: Update model_handler.py to use 224x224 input size:\n",
      "      - Change line 49: image = image.resize((224, 224))\n",
      "\n",
      "   3. Restart your backend server\n",
      "\n",
      "COMPARISON:\n",
      "   • Previous MobileNetV2 (192x192): ~90% accuracy\n",
      "   • New EfficientNetB0 (224x224): 16.45% accuracy\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training script completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# COMPLETE! - Summary and Next Steps\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "TRAINING SUMMARY:\n",
    "   • Model: EfficientNetB0 (Transfer Learning from ImageNet)\n",
    "   • Image Size: {IMG_SIZE}x{IMG_SIZE}\n",
    "   • Final Accuracy: {val_accuracy*100:.2f}%\n",
    "   • Best Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\n",
    "   • Epochs Trained: {len(history.history['accuracy'])} / {EPOCHS}\n",
    "   • Training Time: {training_duration}\n",
    "\n",
    "SAVED FILES:\n",
    "   1. {MODEL_SAVE_PATH}\n",
    "      - The trained model file\n",
    "   2. {HISTORY_PATH}\n",
    "      - Training metrics in JSON format\n",
    "\n",
    "TO USE THE NEW MODEL:\n",
    "   1. Copy the new model to the backend folder:\n",
    "      \n",
    "      copy \"{MODEL_SAVE_PATH}\" \"{os.path.dirname(MODEL_SAVE_PATH)}\\\\backend\\\\trained_model_fito.h5\"\n",
    "      \n",
    "   2. IMPORTANT: Update model_handler.py to use 224x224 input size:\n",
    "      - Change line 49: image = image.resize((224, 224))\n",
    "      \n",
    "   3. Restart your backend server\n",
    "\n",
    "COMPARISON:\n",
    "   • Previous MobileNetV2 (192x192): ~90% accuracy\n",
    "   • New EfficientNetB0 (224x224): {val_accuracy*100:.2f}% accuracy\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training script completed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a43bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 6: SAVE TRAINING HISTORY - Save metrics for documentation\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: SAVING - Saving Training History\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save training history as JSON\n",
    "history_dict = {\n",
    "    'accuracy': [float(x) for x in history.history['accuracy']],\n",
    "    'loss': [float(x) for x in history.history['loss']],\n",
    "    'val_accuracy': [float(x) for x in history.history['val_accuracy']],\n",
    "    'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "    'final_val_accuracy': float(val_accuracy),\n",
    "    'final_val_loss': float(val_loss),\n",
    "    'training_duration': str(training_duration),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'model': 'EfficientNetB0',\n",
    "        'img_size': IMG_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'epochs_trained': len(history.history['accuracy']),\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'training_samples': train_generator.samples,\n",
    "        'validation_samples': val_generator.samples,\n",
    "        'num_classes': train_generator.num_classes,\n",
    "        'classes': list(train_generator.class_indices.keys())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(HISTORY_PATH, 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "    \n",
    "print(f\"Training history saved to: {HISTORY_PATH}\")\n",
    "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
