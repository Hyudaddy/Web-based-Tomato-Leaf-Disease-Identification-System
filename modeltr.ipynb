{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b16fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# IMPORTS - Required libraries for deep learning and data processing\n",
    "# ================================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, SpatialDropout2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9280617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# OUTDOOR NOISE PREPROCESSING FUNCTION\n",
    "# ================================================================================\n",
    "def add_outdoor_noise(image):\n",
    "    \"\"\"\n",
    "    Simulate outdoor conditions: noise, blur, contrast variations\n",
    "    This helps the model generalize to real outdoor images with:\n",
    "    - Camera shake and wind (Gaussian noise)\n",
    "    - Motion blur and focus issues (Gaussian blur)\n",
    "    - Varying lighting conditions (contrast adjustments)\n",
    "    \"\"\"\n",
    "    # Add Gaussian noise (wind, camera shake) - 30% chance\n",
    "    if np.random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 0.02, image.shape)\n",
    "        image = np.clip(image + noise, 0, 1)\n",
    "    \n",
    "    # Add slight blur (motion/focus issues) - 20% chance\n",
    "    if np.random.random() < 0.2:\n",
    "        sigma = np.random.uniform(0.5, 1.5)\n",
    "        # Apply blur to each channel separately\n",
    "        for i in range(image.shape[2]):\n",
    "            image[:, :, i] = gaussian_filter(image[:, :, i], sigma=sigma)\n",
    "    \n",
    "    # Random contrast adjustment - 30% chance\n",
    "    if np.random.random() < 0.3:\n",
    "        alpha = np.random.uniform(0.8, 1.2)\n",
    "        image = np.clip(alpha * image, 0, 1)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe3f1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring TensorFlow for Intel Core i5-13420H...\n",
      "✓ TensorFlow optimizations applied\n",
      "  • Intra-op parallelism: 8 threads\n",
      "  • Inter-op parallelism: 2 threads\n",
      "  • Intel oneDNN: Enabled\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# CPU OPTIMIZATIONS - Configure TensorFlow for maximum performance\n",
    "# ================================================================================\n",
    "print(\"Configuring TensorFlow for Intel Core i5-13420H...\")\n",
    "\n",
    "# Enable Intel oneDNN optimizations (significant speedup on Intel CPUs)\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "# Configure threading for 8-core CPU (4 P-cores + 4 E-cores)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)  # Use all 8 cores\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)  # Parallel operations\n",
    "\n",
    "# Disable GPU (force CPU usage)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "print(\"✓ TensorFlow optimizations applied\")\n",
    "print(f\"  • Intra-op parallelism: 8 threads\")\n",
    "print(f\"  • Inter-op parallelism: 2 threads\")\n",
    "print(f\"  • Intel oneDNN: Enabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a5ae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOMATO LEAF DISEASE DETECTION - OUTDOOR + CLASS BALANCE OPTIMIZED\n",
      "================================================================================\n",
      "\n",
      "Training Configuration:\n",
      "   • CPU: Intel Core i5-13420H (8 cores)\n",
      "   • RAM: 16GB\n",
      "   • Image Size: 224x224\n",
      "   • Batch Size: 32\n",
      "   • Epochs: 50 (with early stopping)\n",
      "   • Learning Rate: 0.001\n",
      "   • Dataset Path: C:\\Users\\altai\\Desktop\\DATASET\\tomato leaf diseases dataset(augmented)\n",
      "\n",
      "Special Optimizations:\n",
      "   ✓ Class weight balancing (handles imbalanced datasets)\n",
      "   ✓ Enhanced augmentation for outdoor conditions\n",
      "   ✓ Outdoor noise preprocessing (noise, blur, contrast)\n",
      "   ✓ Increased dropout for better generalization\n",
      "\n",
      "Estimated training time: 30-50 minutes\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 1: CONFIGURATION - Define paths and hyperparameters\n",
    "# ================================================================================\n",
    "# This is where we configure all the settings BEFORE training begins.\n",
    "# The hyperparameters control HOW the model learns.\n",
    "\n",
    "# Dataset paths - Where the images are stored\n",
    "DATASET_PATH = r\"C:\\Users\\altai\\Desktop\\DATASET\\tomato leaf diseases dataset(augmented)\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"training\")     # Training images (80%)\n",
    "VAL_PATH = os.path.join(DATASET_PATH, \"validation\")     # Validation images (20%)\n",
    "MODEL_SAVE_PATH = r\"C:\\Users\\altai\\Desktop\\TLDI_system\\backend\\trained_model_fito_outdoor.h5\"\n",
    "BACKUP_MODEL_PATH = r\"C:\\Users\\altai\\Desktop\\TLDI_system\\backend\\trained_model_fito_outdoor_backup.h5\"\n",
    "HISTORY_PATH = r\"C:\\Users\\altai\\Desktop\\TLDI_system\\training_history_outdoor.json\"\n",
    "\n",
    "# HYPERPARAMETERS - Optimized for Intel Core i5-13420H + Outdoor Images\n",
    "# -------------------------------------------------------------------\n",
    "IMG_SIZE = 224          # Image resolution: 224x224 pixels (MobileNetV2 optimal size)\n",
    "BATCH_SIZE = 32         # Number of images processed together (optimized for 8 cores)\n",
    "EPOCHS = 50             # Number of complete passes through the entire dataset\n",
    "LEARNING_RATE = 0.001   # Learning rate (higher for faster convergence with larger batches)\n",
    "VALIDATION_SPLIT = 0.2  # 20% of data used for validation\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOMATO LEAF DISEASE DETECTION - OUTDOOR + CLASS BALANCE OPTIMIZED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"   • CPU: Intel Core i5-13420H (8 cores)\")\n",
    "print(f\"   • RAM: 16GB\")\n",
    "print(f\"   • Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   • Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   • Epochs: {EPOCHS} (with early stopping)\")\n",
    "print(f\"   • Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   • Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"\\nSpecial Optimizations:\")\n",
    "print(f\"   ✓ Class weight balancing (handles imbalanced datasets)\")\n",
    "print(f\"   ✓ Enhanced augmentation for outdoor conditions\")\n",
    "print(f\"   ✓ Outdoor noise preprocessing (noise, blur, contrast)\")\n",
    "print(f\"   ✓ Increased dropout for better generalization\")\n",
    "print(f\"\\nEstimated training time: 30-50 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271177b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: Configuring enhanced data augmentation for outdoor conditions...\n",
      "================================================================================\n",
      "\n",
      "Loading training data...\n",
      "Found 20452 images belonging to 11 classes.\n",
      "✓ Training data loaded successfully\n",
      "Loading validation data...\n",
      "Found 5488 images belonging to 11 classes.\n",
      "✓ Validation data loaded successfully\n",
      "\n",
      "Dataset Summary:\n",
      "   • Training samples: 20452\n",
      "   • Validation samples: 5488\n",
      "   • Number of classes: 11\n",
      "   • Classes: ['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'Unidentified']\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 2: DATA PREPARATION - Configure how images are loaded and augmented\n",
    "# ================================================================================\n",
    "# Enhanced data augmentation for OUTDOOR CONDITIONS\n",
    "# This simulates real-world outdoor images with varying lighting, angles, and noise\n",
    "\n",
    "# TRAINING DATA AUGMENTATION - ENHANCED FOR OUTDOOR CONDITIONS\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Configuring enhanced data augmentation for outdoor conditions...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,                  # Normalize pixel values from 0-255 to 0-1\n",
    "    rotation_range=40,               # ↑ from 25 (more camera angles)\n",
    "    width_shift_range=0.3,           # ↑ from 0.25 (off-center shots)\n",
    "    height_shift_range=0.3,          # ↑ from 0.25\n",
    "    shear_range=0.3,                 # ↑ from 0.25 (perspective variations)\n",
    "    zoom_range=0.35,                 # ↑ from 0.25 (closer/farther shots)\n",
    "    brightness_range=[0.5, 1.5],     # ↑ from [0.7, 1.3] (harsh sunlight/shadows)\n",
    "    channel_shift_range=30.0,        # NEW: Color temperature variations (outdoor lighting)\n",
    "    horizontal_flip=True,            # Flip images horizontally\n",
    "    vertical_flip=True,              # Flip images vertically\n",
    "    fill_mode='reflect',             # Changed from 'nearest' (better for outdoor backgrounds)\n",
    "    preprocessing_function=add_outdoor_noise  # NEW: Simulate outdoor noise/blur\n",
    ")\n",
    "\n",
    "# VALIDATION DATA - Only normalize, NO augmentation (to test on real data)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# LOAD TRAINING DATA - Read images from the training folder\n",
    "print(\"\\nLoading training data...\")\n",
    "try:\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_PATH,                         # Path to training images folder\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),   # Resize all images to 224x224\n",
    "        batch_size=BATCH_SIZE,              # Load 32 images at a time\n",
    "        class_mode='categorical',           # Multi-class classification (one-hot)\n",
    "        shuffle=True                        # Randomize order each epoch\n",
    "    )\n",
    "    print(f\"✓ Training data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training data: {e}\")\n",
    "    print(f\"Please ensure training data exists at: {TRAIN_PATH}\")\n",
    "    raise\n",
    "\n",
    "# LOAD VALIDATION DATA - Used to check model performance during training\n",
    "print(\"Loading validation data...\")\n",
    "try:\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        VAL_PATH,                           # Path to validation images folder\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),   # Same size as training\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False                       # Don't shuffle for consistent evaluation\n",
    "    )\n",
    "    print(f\"✓ Validation data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading validation data: {e}\")\n",
    "    print(f\"Please ensure validation data exists at: {VAL_PATH}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"   • Training samples: {train_generator.samples}\")\n",
    "print(f\"   • Validation samples: {val_generator.samples}\")\n",
    "print(f\"   • Number of classes: {train_generator.num_classes}\")\n",
    "print(f\"   • Classes: {list(train_generator.class_indices.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3e3efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: Calculating class weights to handle imbalanced dataset...\n",
      "================================================================================\n",
      "\n",
      "Class Distribution and Weights:\n",
      "--------------------------------------------------------------------------------\n",
      "Class Name                                          Samples   Weight      %\n",
      "--------------------------------------------------------------------------------\n",
      "Tomato___Bacterial_spot                                1702    1.092   8.3%\n",
      "Tomato___Early_blight                                  1920    0.968   9.4%\n",
      "Tomato___Late_blight                                   1851    1.004   9.1%\n",
      "Tomato___Leaf_Mold                                     1882    0.988   9.2%\n",
      "Tomato___Septoria_leaf_spot                            1745    1.065   8.5%\n",
      "Tomato___Spider_mites Two-spotted_spider_mite          1741    1.068   8.5%\n",
      "Tomato___Target_Spot                                   1827    1.018   8.9%\n",
      "Tomato___Tomato_Yellow_Leaf_Curl_Virus                 1961    0.948   9.6%\n",
      "Tomato___Tomato_mosaic_virus                           1790    1.039   8.8%\n",
      "Tomato___healthy                                       1926    0.965   9.4%\n",
      "Unidentified                                           2107    0.882  10.3%\n",
      "--------------------------------------------------------------------------------\n",
      "TOTAL                                                 20452\n",
      "\n",
      "Imbalance Analysis:\n",
      "   • Maximum samples per class: 2107\n",
      "   • Minimum samples per class: 1702\n",
      "   • Imbalance ratio: 1.24:1\n",
      "Dataset is relatively balanced\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 3: CLASS WEIGHT CALCULATION - Handle imbalanced datasets\n",
    "# ================================================================================\n",
    "# Calculate class weights to ensure all classes get equal attention during training\n",
    "# This prevents the model from being biased toward majority classes\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Calculating class weights to handle imbalanced dataset...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count samples per class\n",
    "class_counts = {}\n",
    "for class_name in train_generator.class_indices.keys():\n",
    "    class_path = os.path.join(TRAIN_PATH, class_name)\n",
    "    count = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n",
    "    class_counts[class_name] = count\n",
    "\n",
    "# Calculate class weights using inverse frequency\n",
    "total_samples = train_generator.samples\n",
    "num_classes = train_generator.num_classes\n",
    "class_weights = {}\n",
    "\n",
    "print(\"\\nClass Distribution and Weights:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Class Name':<50} {'Samples':>8} {'Weight':>8} {'%':>6}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for class_name, class_idx in sorted(train_generator.class_indices.items(), key=lambda x: x[1]):\n",
    "    count = class_counts[class_name]\n",
    "    # Weight = total_samples / (num_classes * class_count)\n",
    "    weight = total_samples / (num_classes * count)\n",
    "    class_weights[class_idx] = weight\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"{class_name:<50} {count:>8} {weight:>8.3f} {percentage:>5.1f}%\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TOTAL':<50} {total_samples:>8}\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "max_samples = max(class_counts.values())\n",
    "min_samples = min(class_counts.values())\n",
    "imbalance_ratio = max_samples / min_samples\n",
    "\n",
    "print(f\"\\nImbalance Analysis:\")\n",
    "print(f\"   • Maximum samples per class: {max_samples}\")\n",
    "print(f\"   • Minimum samples per class: {min_samples}\")\n",
    "print(f\"   • Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 2.0:\n",
    "    print(f\"   ⚠️  SIGNIFICANT imbalance detected - Class weights will help!\")\n",
    "elif imbalance_ratio > 1.5:\n",
    "    print(f\"   ⚠️  MODERATE imbalance detected - Class weights recommended\")\n",
    "else:\n",
    "    print(f\"Dataset is relatively balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ba559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: Building model with MobileNetV2 transfer learning...\n",
      "================================================================================\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 7us/step\n",
      "\n",
      "Model Architecture:\n",
      "   • Base Model: MobileNetV2 (frozen)\n",
      "   • Input Shape: (224, 224, 3)\n",
      "   • Custom Layers: SpatialDropout2D(0.2) → Dense(512) → Dense(256) → Dense(11)\n",
      "   • Dropout Rates: 0.4, 0.4, 0.3 (increased for outdoor robustness)\n",
      "   • Total Parameters: 3,048,011\n",
      "   • Trainable Parameters: 790,027\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 4: MODEL BUILDING - Create the neural network architecture\n",
    "# ================================================================================\n",
    "# We use TRANSFER LEARNING: start with a pre-trained model (MobileNetV2)\n",
    "# that already knows how to extract image features, then add our own\n",
    "# classification layers on top for tomato disease detection.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: Building model with MobileNetV2 transfer learning...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# STEP 4A: Load pre-trained MobileNetV2 (trained on ImageNet - 1M+ images)\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),  # 224x224 RGB images\n",
    "    include_top=False,                     # Remove original classification layer\n",
    "    weights='imagenet'                     # Use pre-trained ImageNet weights\n",
    ")\n",
    "\n",
    "# STEP 4B: Freeze base model - Don't retrain the pre-trained layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# STEP 4C: Add custom classification layers with INCREASED DROPOUT\n",
    "# Higher dropout helps prevent overfitting to specific backgrounds\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base_model(inputs, training=False)    # Pass through MobileNetV2\n",
    "x = SpatialDropout2D(0.2)(x)              # NEW: Spatial dropout before pooling\n",
    "x = GlobalAveragePooling2D()(x)           # Reduce feature maps to 1D\n",
    "x = Dropout(0.4)(x)                       # ↑ from 0.3 (more robust)\n",
    "x = Dense(512, activation='relu')(x)      # Fully connected layer: 512 neurons\n",
    "x = Dropout(0.4)(x)                       # ↑ from 0.3 (more robust)\n",
    "x = Dense(256, activation='relu')(x)      # Fully connected layer: 256 neurons\n",
    "x = Dropout(0.3)(x)                       # ↑ from 0.2 (more robust)\n",
    "outputs = Dense(train_generator.num_classes, activation='softmax')(x)  # Output layer\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# STEP 4D: Compile model - Define how it learns\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),  # Optimizer: how weights are updated\n",
    "    loss='categorical_crossentropy',               # Loss function for multi-class\n",
    "    metrics=['accuracy']                           # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"   • Base Model: MobileNetV2 (frozen)\")\n",
    "print(f\"   • Input Shape: ({IMG_SIZE}, {IMG_SIZE}, 3)\")\n",
    "print(f\"   • Custom Layers: SpatialDropout2D(0.2) → Dense(512) → Dense(256) → Dense({train_generator.num_classes})\")\n",
    "print(f\"   • Dropout Rates: 0.4, 0.4, 0.3 (increased for outdoor robustness)\")\n",
    "print(f\"   • Total Parameters: {model.count_params():,}\")\n",
    "print(f\"   • Trainable Parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7551f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: Starting training with class weights and outdoor augmentation...\n",
      "================================================================================\n",
      "Training on Intel Core i5-13420H (8 cores)...\n",
      "Estimated time: 30-50 minutes (outdoor preprocessing adds ~20% time)\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m 48/640\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:53\u001b[0m 800ms/step - accuracy: 0.3652 - loss: 1.7222"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 5: TRAINING - Train the model on the dataset WITH CLASS WEIGHTS\n",
    "# ================================================================================\n",
    "# This is the main training loop where the model learns from the images.\n",
    "# Class weights ensure all classes get equal attention during training.\n",
    "\n",
    "# CALLBACKS - Special functions that run during training\n",
    "callbacks = [\n",
    "    # ModelCheckpoint: Save the model whenever validation accuracy improves\n",
    "    ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        monitor='val_accuracy',    # Watch validation accuracy\n",
    "        save_best_only=True,       # Only save if it's the best so far\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    # EarlyStopping: Stop training if model stops improving (prevents overfitting)\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,                # Stop if no improvement for 7 epochs\n",
    "        restore_best_weights=True, # Go back to the best weights\n",
    "        verbose=1\n",
    "    ),\n",
    "    # ReduceLROnPlateau: Lower learning rate if training plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,                # Reduce LR by half\n",
    "        patience=3,                # Wait 3 epochs before reducing\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# *** THIS IS WHERE THE ACTUAL TRAINING HAPPENS ***\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: Starting training with class weights and outdoor augmentation...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Training on Intel Core i5-13420H (8 cores)...\")\n",
    "print(\"Estimated time: 30-50 minutes (outdoor preprocessing adds ~20% time)\\n\")\n",
    "\n",
    "# model.fit() - THE MAIN TRAINING FUNCTION WITH CLASS WEIGHTS\n",
    "# Note: workers and use_multiprocessing removed for Keras 3.x compatibility\n",
    "history = model.fit(\n",
    "    train_generator,              # Training data\n",
    "    epochs=EPOCHS,                # Train for 50 epochs (or until early stop)\n",
    "    validation_data=val_generator,  # Validation data to monitor overfitting\n",
    "    class_weight=class_weights,   # *** CLASS WEIGHTS FOR BALANCED TRAINING ***\n",
    "    callbacks=callbacks,          # Run checkpoint, early stop, LR reduction\n",
    "    verbose=1                     # Show progress bar\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8fad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: Evaluating model on validation set...\n",
      "================================================================================\n",
      "\u001b[1m 27/172\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 341ms/step - accuracy: 0.7812 - loss: 0.7390"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Evaluate the model on validation data (images it hasn't trained on)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m val_loss, val_accuracy = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:511\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    510\u001b[39m     callbacks.on_test_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     callbacks.on_test_batch_end(end_step, logs)\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_evaluating:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 6: VALIDATION/TESTING - Evaluate the trained model\n",
    "# ================================================================================\n",
    "# After training, we evaluate the model on the validation set one final time\n",
    "# to get the official accuracy metrics.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: Evaluating model on validation set...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate the model on validation data (images it hasn't trained on)\n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 7: SAVING - Save the trained model and training history\n",
    "# ================================================================================\n",
    "# Save the model file (.h5) and training metrics for later analysis.\n",
    "\n",
    "print(f\"\\nSTEP 7: Saving training history...\")\n",
    "history_dict = {\n",
    "    'accuracy': [float(x) for x in history.history['accuracy']],\n",
    "    'loss': [float(x) for x in history.history['loss']],\n",
    "    'val_accuracy': [float(x) for x in history.history['val_accuracy']],\n",
    "    'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "    'final_val_accuracy': float(val_accuracy),\n",
    "    'final_val_loss': float(val_loss),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'cpu': 'Intel Core i5-13420H (8 cores)',\n",
    "        'ram': '16GB',\n",
    "        'img_size': IMG_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'training_samples': train_generator.samples,\n",
    "        'validation_samples': val_generator.samples,\n",
    "        'num_classes': train_generator.num_classes,\n",
    "        'classes': list(train_generator.class_indices.keys()),\n",
    "        'class_weights': {str(k): float(v) for k, v in class_weights.items()},\n",
    "        'imbalance_ratio': float(imbalance_ratio),\n",
    "        'optimizations': [\n",
    "            'class_weight_balancing',\n",
    "            'outdoor_augmentation',\n",
    "            'outdoor_noise_preprocessing',\n",
    "            'increased_dropout',\n",
    "            'spatial_dropout'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(HISTORY_PATH, 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "print(f\"History saved to: {HISTORY_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel saved at: {MODEL_SAVE_PATH}\")\n",
    "print(f\"\\nFinal Training Results:\")\n",
    "print(f\"   • Validation Accuracy: {val_accuracy:.2%}\")\n",
    "print(f\"   • Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"   • Total Epochs Trained: {len(history.history['accuracy'])}\")\n",
    "print(f\"   • Best Epoch: {history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1}\")\n",
    "\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "print(f\"   1. Test the model with OUTDOOR tomato leaf images\")\n",
    "print(f\"   2. Compare performance with previous model (trained_model_fito.h5)\")\n",
    "print(f\"   3. Update backend to use new model:\")\n",
    "print(f\"      - Rename: trained_model_fito_outdoor.h5 → trained_model_fito.h5\")\n",
    "print(f\"   4. Review training history at: {HISTORY_PATH}\")\n",
    "\n",
    "print(f\"\\nOptimizations Applied:\")\n",
    "print(f\"   ✓ Class weight balancing (imbalance ratio: {imbalance_ratio:.2f}:1)\")\n",
    "print(f\"   ✓ Enhanced outdoor augmentation (rotation, zoom, brightness)\")\n",
    "print(f\"   ✓ Outdoor noise preprocessing (noise, blur, contrast)\")\n",
    "print(f\"   ✓ Increased dropout (0.4, 0.4, 0.3) for robustness\")\n",
    "print(f\"   ✓ Spatial dropout before pooling\")\n",
    "print(f\"   ✓ Intel oneDNN + 8-core parallelization\")\n",
    "\n",
    "print(f\"\\nExpected Improvements:\")\n",
    "print(f\"   • 30-50% better accuracy on outdoor images\")\n",
    "print(f\"   • Balanced performance across all disease classes\")\n",
    "print(f\"   • Better handling of harsh lighting and shadows\")\n",
    "print(f\"   • More robust to different backgrounds and angles\")\n",
    "print(f\"   • Reduced bias toward majority classes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
